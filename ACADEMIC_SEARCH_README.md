# ğŸ”¬ Academic Search Integration

Sistema di ricerca accademica integrato per verifica di claim scientifici e letteratura.

## ğŸ“š **Cosa Fa**

Due livelli di ricerca accademica:

### **1. Subject Matter Expert con Web Search** (Tier 2)
- âœ… Sempre attivo
- ğŸŒ Usa OpenAI Responses API + Tavily
- ğŸ¯ Verifica claim tecnici
- ğŸ“Š Controlla standard di settore
- ğŸ” Trova sviluppi recenti

### **2. Academic Researcher** (Tier 3, `--deep-review`)
- ğŸ”¬ Semantic Scholar API per papers accademici
- ğŸŒ Web search per novitÃ  recenti
- ğŸ“– Citazioni complete con DOI/arXiv
- ğŸ“ˆ Analisi citazioni e influenza
- ğŸ§ª Confronto metodologie
- ğŸ“š Gap nella letteratura

---

## ğŸš€ **Setup**

### **1. Installazione**

```bash
# Semantic Scholar Ã¨ incluso nel progetto
# Nessuna installazione aggiuntiva necessaria!

# Opzionale: API key per rate limits piÃ¹ alti
export SEMANTIC_SCHOLAR_API_KEY="your_key_here"
```

### **2. Dipendenze**

```bash
# GiÃ  installate con requirements base
pip install requests
```

---

## ğŸ“– **Utilizzo**

### **ModalitÃ  Standard** (Subject Matter Expert con web search)

```bash
python generic_reviewer.py document.pdf --output-language English
```

**Cosa succede:**
- âœ… `subject_matter_expert` attivo automaticamente (Tier 2)
- ğŸŒ Usa web search per verifiche
- ğŸ¯ Ottimo per documenti tecnici/business

**Log:**
```
ğŸŒ Executing Subject Matter Expert with WEB SEARCH
âœ… Subject Matter Expert OpenAI web search completed successfully
```

### **ModalitÃ  Deep Review** (+ Academic Researcher)

```bash
python generic_reviewer.py document.pdf --deep-review --output-language English
```

**Cosa succede:**
- âœ… Tutti gli agenti Tier 1 + Tier 2 attivi
- ğŸ”¬ `academic_researcher` attivo (Tier 3)
- ğŸ“š Semantic Scholar + Web Search combinati
- ğŸ“– Citazioni accademiche complete

**Log:**
```
[TIER 3] Creating 15 deep-dive specialists (--deep-review active)
ğŸ”¬ Executing Academic Researcher with ACADEMIC SEARCH
ğŸ“š Found 8 papers for query: 'machine learning transformers'
âœ… Semantic Scholar search completed for Academic Researcher
âœ… Web search also completed for Academic Researcher
```

---

## ğŸ¯ **Esempi Pratici**

### **Esempio 1: Paper Scientifico**

```bash
python generic_reviewer.py research_paper.pdf --deep-review
```

**Output:**
- ğŸ“Š Subject Matter Expert verifica accuracy tecnica
- ğŸ”¬ Academic Researcher trova 10 papers correlati
- ğŸ“– Citazioni con DOI, arXiv, conteggio citazioni
- ğŸ§ª Confronto metodologie con letteratura recente
- âš ï¸ Identificazione gap o claim non supportati

### **Esempio 2: Business Proposal**

```bash
python generic_reviewer.py proposal.pdf
```

**Output:**
- âœ… Subject Matter Expert verifica best practices
- ğŸŒ Web search per trend di mercato recenti
- âŒ Academic Researcher NON attivo (non necessario)

### **Esempio 3: Technical Document**

```bash
python generic_reviewer.py technical_spec.docx --deep-review
```

**Output:**
- âœ… Subject Matter Expert + Web Search (standard tecnici)
- ğŸ”¬ Academic Researcher + Semantic Scholar (ricerca cutting-edge)
- ğŸ“ˆ Papers su tecnologie emergenti
- ğŸ”— Link a implementazioni di riferimento

---

## ğŸ“Š **Cosa Fornisce Academic Researcher**

### **Per ogni paper trovato:**

```markdown
### [1] Attention Is All You Need
**Authors:** Vaswani, Ashish et al.
**Published:** NeurIPS 2017
**Citations:** 85432 (Influential: 12543)
**Fields:** Computer Science, Machine Learning, NLP
**DOI:** [10.5555/3295222.3295349](https://doi.org/10.5555/3295222.3295349)
**arXiv:** [1706.03762](https://arxiv.org/abs/1706.03762)
**Abstract:** The dominant sequence transduction models are based on 
complex recurrent or convolutional neural networks...
```

### **Analisi dell'agente:**

```markdown
## Literature Alignment
âœ… Claim about transformer efficiency is supported by [1] Vaswani et al. (2017)
âš ï¸ Performance numbers differ from [3] recent benchmark (2024)
âŒ No citation for "98% accuracy" - not found in literature

## Methodology Comparison
Your approach uses method X, similar to [2] but differs in:
- Parameter initialization strategy
- Training data scale (yours: 1M samples, theirs: 10M)

## Research Gaps
ğŸ” No recent work (2023-2025) addresses your specific use case
ğŸ’¡ Consider citing emerging work on efficient transformers [4,5]

## Suggested References
ğŸ“– Add: [6] "Efficient Transformers: A Survey" (2022, 3400 citations)
ğŸ“– Consider: [7] "Recent Advances in NLP" (2024, 150 citations)
```

---

## ğŸ›ï¸ **Configurazione Agenti**

### **generic_reviewer.py** - Configurazione

```python
# TIER 2: Subject Matter Expert (sempre attivo)
"subject_matter_expert": {
    "name": "Subject Matter Expert",
    "icon": "ğŸ“",
    "instructions": """...""",
    "use_web_search": True  # â† Web search enabled
}

# TIER 3: Academic Researcher (solo --deep-review)
"academic_researcher": {
    "name": "Academic Researcher",
    "icon": "ğŸ”¬",
    "instructions": """...""",
    "use_academic_search": True,  # â† Semantic Scholar
    "use_web_search": True        # â† + Web search
}
```

### **Model Selection (Cost-Optimized)**

```python
AGENT_COMPLEXITY = {
    "subject_matter_expert": 0.9,   # High complexity
    "academic_researcher": 0.9,     # High complexity
}

# Con documento complexity 0.7:
# final_score = 0.7 * 0.4 + 0.9 * 0.6 = 0.82
# â†’ gpt-5 (threshold >= 0.80)
```

**Entrambi usano GPT-5** per garantire qualitÃ  della ricerca!

---

## ğŸ” **Flusso di Ricerca**

### **Academic Researcher Workflow:**

```
1. ğŸ”¬ SEMANTIC SCHOLAR API
   â”œâ”€ Search papers (last 5 years)
   â”œâ”€ Get top 10 by citations
   â”œâ”€ Extract metadata (DOI, arXiv, authors, venue)
   â””â”€ Format for agent context

2. ğŸŒ WEB SEARCH (optional)
   â”œâ”€ OpenAI Responses API
   â”œâ”€ Query: "recent developments in [topic]"
   â”œâ”€ Timeout: 60 seconds
   â””â”€ Combine with academic results

3. ğŸ¤– AGENT ANALYSIS
   â”œâ”€ Receive: document + research data
   â”œâ”€ Verify claims against papers
   â”œâ”€ Identify gaps & conflicts
   â””â”€ Suggest additional references
```

### **Fallback Strategy:**

```
Semantic Scholar
   â”œâ”€ Success â†’ Continue to web search
   â””â”€ Fail â†’ Log warning, continue anyway

Web Search
   â”œâ”€ Success â†’ Combine results
   â”œâ”€ Timeout (60s) â†’ Use only Semantic Scholar
   â””â”€ Fail â†’ Use only Semantic Scholar

Combined Results
   â”œâ”€ Has results â†’ Agent analyzes
   â””â”€ No results â†’ Standard execution (no research data)
```

---

## ğŸ“ˆ **Rate Limits & Performance**

### **Semantic Scholar API**

| Tier | Rate Limit | Setup |
|------|-----------|-------|
| **Free** | 1 req/sec | Nessuna API key necessaria |
| **With Key** | 10 req/sec | `export SEMANTIC_SCHOLAR_API_KEY="..."` |

**Nel codice:**
- âœ… Rate limiting automatico
- âœ… Retry logic
- âœ… Timeout handling
- âœ… Graceful degradation

### **Performance Tipica**

```
ğŸ“Š Review Standard (15 agenti):
- Subject Matter Expert: +30s (web search)
- Totale: ~3-5 minuti

ğŸ”¬ Deep Review (25 agenti + academic):
- Academic Researcher: +60s (Semantic Scholar + Web)
- Totale: ~8-12 minuti
```

---

## ğŸ“ **Semantic Scholar Features**

### **Supporta:**

âœ… **Paper Search**: keywords, phrases, Boolean queries
âœ… **Metadata Completi**: DOI, arXiv, venue, fields of study
âœ… **Citation Data**: citation count, influential citations
âœ… **Author Info**: author names and IDs
âœ… **Related Papers**: recommendations algorithm
âœ… **References & Citations**: chi cita, chi Ã¨ citato
âœ… **Year Filtering**: limita per range temporale
âœ… **Field Filtering**: computer science, biology, etc.

### **Database Coverage:**

- ğŸ“š **200M+ papers**
- ğŸ”¬ **All fields**: CS, Physics, Medicine, Biology, etc.
- ğŸ“– **Sources**: arXiv, PubMed, IEEE, ACM, Springer, etc.
- ğŸ†“ **Free**: no registration required
- ğŸš€ **Fast**: optimized API endpoints

---

## ğŸ› ï¸ **Testing**

### **Test Semantic Scholar Module:**

```bash
python semantic_scholar.py
```

**Output:**
```
ğŸ§ª Testing Semantic Scholar API...

ğŸ“š Test 1: Searching papers about 'transformers in NLP'...
Found 5 papers

Top result:
Vaswani, Ashish et al. (2017). Attention Is All You Need. [85432 citations]
  Published in: NeurIPS
  arXiv: 1706.03762
  URL: https://arxiv.org/abs/1706.03762

ğŸ“„ Test 2: Getting paper by DOI...
Found: BERT: Pre-training of Deep Bidirectional Transformers
Citations: 54321

ğŸ“ Test 3: Formatting papers for agent context...
## Academic Research Results (3 papers)

### [1] Attention Is All You Need
**Authors:** Vaswani, Ashish et al.
...

âœ… All tests completed!
```

### **Test con Generic Reviewer:**

```bash
# Test senza Semantic Scholar (fallback)
python generic_reviewer.py test.pdf --deep-review

# Test con Semantic Scholar
python generic_reviewer.py test.pdf --deep-review
# Watch logs for: ğŸ”¬ Executing Academic Researcher with ACADEMIC SEARCH
```

---

## ğŸ†š **Confronto Agenti**

| Feature | Subject Matter Expert | Academic Researcher |
|---------|----------------------|---------------------|
| **Tier** | 2 (sempre attivo) | 3 (solo `--deep-review`) |
| **Model** | gpt-5 / gpt-5-mini | gpt-5 (sempre) |
| **Web Search** | âœ… OpenAI + Tavily | âœ… OpenAI + Tavily |
| **Academic DB** | âŒ | âœ… Semantic Scholar |
| **Citazioni** | Informali | Formali (DOI/arXiv) |
| **Focus** | Verifica tecnica | Letteratura accademica |
| **Output** | Best practices | Papers + citazioni |
| **Use Case** | Business/Tech docs | Scientific papers |
| **Costo** | ~0.76 final_score | ~0.82 final_score |

---

## ğŸ’¡ **Best Practices**

### **Quando Usare `--deep-review`:**

âœ… **SI** per:
- ğŸ“„ Scientific papers
- ğŸ“ Thesis/dissertation
- ğŸ”¬ Research proposals
- ğŸ“Š Technical reports con claim scientifici
- ğŸ§ª Literature reviews

âŒ **NO** per:
- ğŸ’¼ Business proposals (troppo lento)
- ğŸ“§ Emails/blog posts
- ğŸ“ Marketing copy
- ğŸ¨ Creative writing
- âš¡ Quick reviews (usa standard mode)

### **Ottimizzare Performance:**

```bash
# Fast: solo Subject Matter Expert (web search)
python generic_reviewer.py doc.pdf

# Balanced: Tier 1 + 2 (include Subject Matter Expert)
python generic_reviewer.py doc.pdf

# Complete: Tier 1 + 2 + 3 (+ Academic Researcher)
python generic_reviewer.py doc.pdf --deep-review

# Maximum: tutti + iterative + interactive
python generic_reviewer.py doc.pdf --deep-review --iterative --interactive
```

---

## ğŸ”§ **Troubleshooting**

### **"Semantic Scholar not available"**

```bash
# Verifica che semantic_scholar.py esista
ls semantic_scholar.py

# Verifica import
python -c "from semantic_scholar import SemanticScholarAPI; print('OK')"
```

### **"No papers found"**

- âœ… Query troppo specifica â†’ rilassa keywords
- âœ… Year range troppo stretto â†’ rimuovi filtro anno
- âœ… Typo nel query â†’ correggi spelling

### **"Rate limit exceeded"**

```bash
# Ottieni API key gratuita (10x rate limit)
# https://www.semanticscholar.org/product/api

export SEMANTIC_SCHOLAR_API_KEY="your_key"
```

### **Performance lenta**

```bash
# Riduci numero papers
# In generic_reviewer.py, linea ~2714:
academic_result = execute_academic_research(agent.name, query, limit=5)  # â† da 10 a 5

# Oppure disabilita web search per academic researcher
# Rimuovi "use_web_search": True dal template
```

---

## ğŸ“Š **Esempi Output**

### **Subject Matter Expert Review:**

```markdown
## Domain Expertise Analysis

### Technical Accuracy âœ…
Your implementation of transformer attention follows best practices 
as outlined in recent industry standards (source: 
https://arxiv.org/abs/2304.12345, verified via web search).

### Current Best Practices âš ï¸
The paper uses batch size 32, but recent benchmarks (2024) suggest 
batch size 64-128 for optimal performance on modern GPUs.
Source: NeurIPS 2024 proceedings.

### Industry Standards Compliance âœ…
Follows IEEE standards for model documentation (IEEE 2894-2024).

### State-of-the-Art Awareness âš ï¸
No mention of recent FlashAttention2 optimization (March 2024),
which provides 2-3x speedup. Consider adding benchmark comparison.
```

### **Academic Researcher Review:**

```markdown
## Academic Literature Analysis

### Cited Papers Assessment âš ï¸
Your paper cites 15 references:
- âœ… 10 are highly relevant (>1000 citations each)
- âš ï¸ 3 are outdated (pre-2018) - consider updating
- âŒ 2 are tangentially related - reconsider inclusion

### Missing Key Citations ğŸ”
Your work on transformer optimization should cite:

[1] Dao, Tri (2023). FlashAttention-2: Faster Attention with Better 
    Parallelism and Work Partitioning
    **arXiv:** 2307.08691 | **Citations:** 1243
    **Why:** Directly relevant to your optimization claims

[2] Tay, Yi et al. (2022). Efficient Transformers: A Survey
    **DOI:** 10.1145/3530811 | **Citations:** 3421
    **Why:** Comprehensive survey covering your domain

### Methodology Comparison ğŸ“Š
Your approach resembles [3] Zhang et al. (2023) but differs in:
- Training data scale: yours (1M) vs theirs (10M)
- Architecture: you use 12 layers, they use 24

Consider discussing these differences explicitly.

### Literature Gaps Identified ğŸ¯
âœ… No prior work addresses your specific combination of:
   - Low-resource training (<1M samples)
   - Multilingual support (50+ languages)
   - Real-time inference (<10ms)

This is a genuine research contribution!

### Conflicting Evidence âš ï¸
Your claim of "95% accuracy" conflicts with:
- [4] Smith et al. (2024): reports 89% on similar dataset
- [5] Lee et al. (2023): theoretical upper bound of 92%

Recommendation: Re-verify experimental setup or discuss discrepancy.

### Research Trends ğŸ“ˆ
Emerging topics in your field (2024):
- Mixture-of-Experts architectures (trending)
- Quantization for efficiency (hot topic)
- Multimodal transformers (future direction)

Consider positioning your work relative to these trends.
```

---

## ğŸš€ **Roadmap Futuro**

### **Prossime Feature:**

- [ ] PubMed integration (per medicina/biologia)
- [ ] arXiv direct API (per pre-prints recenti)
- [ ] Google Scholar scraping (via SerpAPI)
- [ ] CrossRef DOI lookup
- [ ] Citation network analysis
- [ ] Automated literature review generation
- [ ] Comparative analysis across multiple papers
- [ ] Trend detection in research fields

---

## ğŸ“š **Risorse**

### **Semantic Scholar:**
- ğŸ“– API Docs: https://api.semanticscholar.org/
- ğŸ”‘ API Key: https://www.semanticscholar.org/product/api
- ğŸ’¬ Support: api-support@semanticscholar.org

### **Alternative Academic APIs:**
- ğŸ¥ PubMed: https://www.ncbi.nlm.nih.gov/home/develop/api/
- ğŸ“‘ arXiv: https://arxiv.org/help/api/
- ğŸ” CrossRef: https://www.crossref.org/documentation/retrieve-metadata/rest-api/
- ğŸ“Š OpenCitations: https://opencitations.net/

---

## âœ… **Summary**

### **Cosa Hai Ora:**

1. âœ… **Subject Matter Expert** con web search (Tier 2, sempre attivo)
2. âœ… **Academic Researcher** con Semantic Scholar + Web (Tier 3, `--deep-review`)
3. âœ… **Semantic Scholar API** completamente integrato
4. âœ… **Fallback robusto**: Semantic Scholar â†’ Web Search â†’ Standard
5. âœ… **200M+ papers** accessibili gratuitamente
6. âœ… **Citazioni formali** con DOI/arXiv
7. âœ… **Cost-optimized** ma alta qualitÃ  (entrambi su gpt-5 quando complesso)

### **Comandi Veloci:**

```bash
# Standard: Subject Matter Expert + web search
python generic_reviewer.py doc.pdf

# Deep: + Academic Researcher + Semantic Scholar
python generic_reviewer.py doc.pdf --deep-review

# Test Semantic Scholar
python semantic_scholar.py
```

ğŸ‰ **Sistema di ricerca accademica completo e pronto all'uso!** ğŸ”¬

